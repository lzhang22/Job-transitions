{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = 'Developed data mining applications to scrape data directly from html and plain text. Performed complex SQL queries to aggregate data from disparate sources. Generated databases that exceed - in terms of size and quality - those provided by commercial vendors, giving Odette Faculty a competitive advantage with respect to the originality of their research. Performed ad hod analysis (linear/logistic/multivariate regressions, survival analysis and classification. Communicated findings with visualizations produced using Python data visualization library matplotlib.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = joblib.load('vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = joblib.load('nmf_topic_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('work_exp.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>resume_id</th>\n",
       "      <th>job_type</th>\n",
       "      <th>job_id</th>\n",
       "      <th>job_title</th>\n",
       "      <th>job_duration</th>\n",
       "      <th>job_description</th>\n",
       "      <th>lang</th>\n",
       "      <th>job_title_processed</th>\n",
       "      <th>job_description_processed</th>\n",
       "      <th>Topic 1</th>\n",
       "      <th>...</th>\n",
       "      <th>Topic 9</th>\n",
       "      <th>Topic 10</th>\n",
       "      <th>Topic 11</th>\n",
       "      <th>Topic 12</th>\n",
       "      <th>Topic 13</th>\n",
       "      <th>Topic 14</th>\n",
       "      <th>Topic 15</th>\n",
       "      <th>highest_topic1</th>\n",
       "      <th>highest_topic2</th>\n",
       "      <th>highest_topic3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>data+scientist</td>\n",
       "      <td>0</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>February 2013 to Present</td>\n",
       "      <td>2016 ~ Manager, Smart Solution Team in Informa...</td>\n",
       "      <td>en</td>\n",
       "      <td>data scientist</td>\n",
       "      <td>manager smart solution team information planni...</td>\n",
       "      <td>0.864740</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058644</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035346</td>\n",
       "      <td>0.236468</td>\n",
       "      <td>0.179737</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Topic 1</td>\n",
       "      <td>Topic 7</td>\n",
       "      <td>Topic 13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>data+scientist</td>\n",
       "      <td>1</td>\n",
       "      <td>Business Inteligence Consultant</td>\n",
       "      <td>January 2006 to February 2013</td>\n",
       "      <td>2011 Social Media Analysis- Participate in the...</td>\n",
       "      <td>en</td>\n",
       "      <td>business inteligence consultant</td>\n",
       "      <td>social medium analysis participate development...</td>\n",
       "      <td>0.013170</td>\n",
       "      <td>...</td>\n",
       "      <td>0.380385</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022767</td>\n",
       "      <td>0.604392</td>\n",
       "      <td>0.035650</td>\n",
       "      <td>0.220245</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Topic 12</td>\n",
       "      <td>Topic 5</td>\n",
       "      <td>Topic 9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>data+scientist</td>\n",
       "      <td>0</td>\n",
       "      <td>Director of Business Relations</td>\n",
       "      <td>May 2018 to Present</td>\n",
       "      <td>GIMME360  Data Scientist-Business Solution  20...</td>\n",
       "      <td>en</td>\n",
       "      <td>director business relation</td>\n",
       "      <td>gimme data scientist business solution present...</td>\n",
       "      <td>0.192328</td>\n",
       "      <td>...</td>\n",
       "      <td>0.199249</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.637112</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Topic 12</td>\n",
       "      <td>Topic 8</td>\n",
       "      <td>Topic 7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>data+scientist</td>\n",
       "      <td>1</td>\n",
       "      <td>Senior Data Analyst-Customer Interaction Decis...</td>\n",
       "      <td>May 2014 to July 2016</td>\n",
       "      <td>SAS  Fulltime• Translated business objectives ...</td>\n",
       "      <td>en</td>\n",
       "      <td>senior data analyst customer interaction decis...</td>\n",
       "      <td>sa fulltime translated business objective data...</td>\n",
       "      <td>0.038701</td>\n",
       "      <td>...</td>\n",
       "      <td>0.169296</td>\n",
       "      <td>0.058586</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400573</td>\n",
       "      <td>0.074710</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Topic 7</td>\n",
       "      <td>Topic 12</td>\n",
       "      <td>Topic 9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>data+scientist</td>\n",
       "      <td>0</td>\n",
       "      <td>FREELANCE DEVELOPER</td>\n",
       "      <td>March 2016 to Present</td>\n",
       "      <td>Toronto, Canada• Completed 30+ freelancer Proj...</td>\n",
       "      <td>en</td>\n",
       "      <td>freelance developer</td>\n",
       "      <td>toronto canada completed freelancer project in...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.253182</td>\n",
       "      <td>0.547395</td>\n",
       "      <td>0.134815</td>\n",
       "      <td>0.033566</td>\n",
       "      <td>0.091773</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Topic 7</td>\n",
       "      <td>Topic 11</td>\n",
       "      <td>Topic 10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   resume_id        job_type job_id  \\\n",
       "0          1  data+scientist      0   \n",
       "1          1  data+scientist      1   \n",
       "2          2  data+scientist      0   \n",
       "3          2  data+scientist      1   \n",
       "4          3  data+scientist      0   \n",
       "\n",
       "                                           job_title  \\\n",
       "0                                     Data Scientist   \n",
       "1                    Business Inteligence Consultant   \n",
       "2                     Director of Business Relations   \n",
       "3  Senior Data Analyst-Customer Interaction Decis...   \n",
       "4                                FREELANCE DEVELOPER   \n",
       "\n",
       "                    job_duration  \\\n",
       "0       February 2013 to Present   \n",
       "1  January 2006 to February 2013   \n",
       "2            May 2018 to Present   \n",
       "3          May 2014 to July 2016   \n",
       "4          March 2016 to Present   \n",
       "\n",
       "                                     job_description lang  \\\n",
       "0  2016 ~ Manager, Smart Solution Team in Informa...   en   \n",
       "1  2011 Social Media Analysis- Participate in the...   en   \n",
       "2  GIMME360  Data Scientist-Business Solution  20...   en   \n",
       "3  SAS  Fulltime• Translated business objectives ...   en   \n",
       "4  Toronto, Canada• Completed 30+ freelancer Proj...   en   \n",
       "\n",
       "                                 job_title_processed  \\\n",
       "0                                     data scientist   \n",
       "1                    business inteligence consultant   \n",
       "2                         director business relation   \n",
       "3  senior data analyst customer interaction decis...   \n",
       "4                                freelance developer   \n",
       "\n",
       "                           job_description_processed   Topic 1  \\\n",
       "0  manager smart solution team information planni...  0.864740   \n",
       "1  social medium analysis participate development...  0.013170   \n",
       "2  gimme data scientist business solution present...  0.192328   \n",
       "3  sa fulltime translated business objective data...  0.038701   \n",
       "4  toronto canada completed freelancer project in...  0.000000   \n",
       "\n",
       "        ...         Topic 9  Topic 10  Topic 11  Topic 12  Topic 13  Topic 14  \\\n",
       "0       ...        0.000000  0.058644  0.000000  0.035346  0.236468  0.179737   \n",
       "1       ...        0.380385  0.000000  0.022767  0.604392  0.035650  0.220245   \n",
       "2       ...        0.199249  0.000000  0.000000  0.637112  0.000000  0.000000   \n",
       "3       ...        0.169296  0.058586  0.000000  0.400573  0.074710  0.000000   \n",
       "4       ...        0.000000  0.253182  0.547395  0.134815  0.033566  0.091773   \n",
       "\n",
       "   Topic 15  highest_topic1  highest_topic2  highest_topic3  \n",
       "0       0.0         Topic 1         Topic 7        Topic 13  \n",
       "1       0.0        Topic 12         Topic 5         Topic 9  \n",
       "2       0.0        Topic 12         Topic 8         Topic 7  \n",
       "3       0.0         Topic 7        Topic 12         Topic 9  \n",
       "4       0.0         Topic 7        Topic 11        Topic 10  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \"\"\" preprocess text: remove special characters, remove digits, tokenize,\n",
    "    lowercase, remove stopwords, lemmatize\n",
    "    \"\"\"\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    stopwords_en = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "    text = re.sub('[^a-zA-Z]', ' ', text )\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stopwords_en]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(description, vectorizer, model, df):\n",
    "    \"\"\" calculate cosine similarity between new job description and job descriptions in dataset\n",
    "    \n",
    "    description -- new job description to be analyzed\n",
    "    vectorizer -- vectorizer for text processing\n",
    "    model -- nmf model\n",
    "    df -- df containing resume and job ids and topic weights\n",
    "    \"\"\"\n",
    "    desc = preprocess(description)\n",
    "    desc_vec = vectorizer.transform([desc])\n",
    "    # get normalized topic weights for description\n",
    "    desc_nmf = normalize(model.transform(desc_vec))\n",
    "    \n",
    "    # get topic weights from df\n",
    "    topic_cols = [col for col in df.columns if 'Topic' in col]\n",
    "    norm_features = df.as_matrix(columns=topic_cols)\n",
    "    \n",
    "    # calculate cosine similarity between input description and training descriptions\n",
    "    cos_sim = norm_features.dot(desc_nmf.T)\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim = similarity(description, vectorizer, model, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transitions(description, vectorizer, model, df):\n",
    "    \"\"\" get descriptive info about topic changes at job transitions. \n",
    "    \n",
    "    description -- new job description to be analyzed\n",
    "    vectorizer -- vectorizer for text processing\n",
    "    model -- nmf model\n",
    "    df -- df containing resume and job ids and topic weights\n",
    "    \"\"\"\n",
    "    # find 100 most similar non current job descriptions\n",
    "    df['cos_sim'] = similarity(description, vectorizer, model, df)\n",
    "    df100 = df.loc[df.job_id!=0].sort_values(by=['cos_sim'], ascending=False)[:100]\n",
    "    \n",
    "    # dataframe containing 100 most similar non current jobs and their corresponding subsequent jobs\n",
    "    idx_prev = df100.index.tolist()\n",
    "    idx_current = [i-1 for i in df100.index]\n",
    "    transitions = df[df.index.isin(idx_prev + idx_current)]\n",
    "            \n",
    "    # calculate topic weight difference between job i and job i-1 of same resume\n",
    "    cols = [col for col in transitions.columns if 'Topic' in col]\n",
    "    diff = transitions[cols].diff()\n",
    "    diff = pd.concat([transitions[['resume_id', 'job_id', 'cos_sim']], diff], axis=1, sort=False)\n",
    "    diff['same_resume'] = diff.resume_id.diff() # calculate diff beteen adjacent resume_id, should be 0 if from same resume\n",
    "    diff = diff[diff.same_resume==0]\n",
    "    diff[cols] = diff[cols].apply(lambda x: x*(-1)) # multiply by -1 to get topic change from recent to past job\n",
    "    \n",
    "    # find topic with largest difference as one moves from job i to job i-1,\n",
    "    # compute percentage of people who have highest change in each of the 15 topics\n",
    "    diff1 = diff[cols]\n",
    "    diff1['max_weight_change'] = diff1.idxmax(axis=1)\n",
    "    weight_change = diff1['max_weight_change'].value_counts(normalize=True).rename_axis('topic').reset_index(name='counts')\n",
    "    weight_change['counts'] = round(weight_change['counts'] * 100)\n",
    "    \n",
    "    topic_dict = {'Topic 1':'data warehouse/governance', 'Topic 2':'software development',\n",
    "                  'Topic 3':'database development', 'Topic 4':'network infrastructure',\n",
    "                  'Topic 5':'data analytics and reporting', 'Topic 6':'product testing',\n",
    "                  'Topic 7':'machine learning', 'Topic 8': 'customer support',\n",
    "                  'Topic 9':'business solutions', 'Topic 10':'management/leadership',\n",
    "                  'Topic 11': 'SQL server tools', 'Topic 12': 'marketing',\n",
    "                  'Topic 13': 'academic research', 'Topic 14': 'enterprise data architecture',\n",
    "                  'Topic 15': 'client relationship'}\n",
    "    \n",
    "    largest_change_list = []\n",
    "    for index, row in weight_change.iterrows():\n",
    "        largest_change = [row['counts'], topic_dict[row['topic']]]\n",
    "        largest_change_list.append(largest_change)\n",
    "        \n",
    "    return largest_change_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[12.0, 'management/leadership'],\n",
       " [11.0, 'academic research'],\n",
       " [11.0, 'machine learning'],\n",
       " [10.0, 'data analytics and reporting'],\n",
       " [9.0, 'marketing'],\n",
       " [7.0, 'customer support'],\n",
       " [7.0, 'data warehouse/governance'],\n",
       " [6.0, 'client relationship'],\n",
       " [6.0, 'software development'],\n",
       " [5.0, 'business solutions'],\n",
       " [4.0, 'database development'],\n",
       " [4.0, 'product testing'],\n",
       " [4.0, 'enterprise data architecture'],\n",
       " [3.0, 'network infrastructure'],\n",
       " [1.0, 'SQL server tools']]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transitions(description, vectorizer, model, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_job_examples(description, vectorizer, model, df):\n",
    "    \"\"\" give examples of next jobs and their top 3 topics\n",
    "    \n",
    "    description -- new job description to be analyzed\n",
    "    vectorizer -- vectorizer for text processing\n",
    "    model -- nmf model\n",
    "    df -- df containing resume and job ids and topic weights\n",
    "    \"\"\"\n",
    "    # out of jobs that are not current jobs, 6 jobs with highest cosine similarity\n",
    "    df['cos_sim'] = similarity(description, vectorizer, model, df)\n",
    "    df6 = df.loc[df.job_id!=0].sort_values(by=['cos_sim'], ascending=False)[:6]\n",
    "    \n",
    "    topic_dict = {'Topic 1':'data warehouse/governance', 'Topic 2':'software development',\n",
    "                  'Topic 3':'database development', 'Topic 4':'network infrastructure',\n",
    "                  'Topic 5':'data analytics and reporting', 'Topic 6':'product testing',\n",
    "                  'Topic 7':'machine learning', 'Topic 8': 'customer support',\n",
    "                  'Topic 9':'business solutions', 'Topic 10':'management/leadership',\n",
    "                  'Topic 11': 'SQL server tools', 'Topic 12': 'marketing',\n",
    "                  'Topic 13': 'academic research', 'Topic 14': 'enterprise data architecture',\n",
    "                  'Topic 15': 'client relationship'}\n",
    "    \n",
    "    # for each of 6 most similar jobs, find their subsequent job and top 3 topics\n",
    "    example_list = []\n",
    "    for i in range(6):\n",
    "        resume_id, job_id = df6.iloc[i, :][['resume_id', 'job_id']]\n",
    "        next_job = df.loc[(df['resume_id']==resume_id)&(df['job_id']==job_id-1), 'job_title_processed'].values[0]\n",
    "        highest_topic1 = df.loc[(df['resume_id']==resume_id)&(df['job_id']==job_id-1), 'highest_topic1'].values[0]\n",
    "        highest_topic2 = df.loc[(df['resume_id']==resume_id)&(df['job_id']==job_id-1), 'highest_topic2'].values[0]\n",
    "        highest_topic3 = df.loc[(df['resume_id']==resume_id)&(df['job_id']==job_id-1), 'highest_topic3'].values[0]\n",
    "        example = [next_job, topic_dict[highest_topic1], topic_dict[highest_topic2], topic_dict[highest_topic3]]\n",
    "        example_list.append(example)\n",
    "    \n",
    "    return example_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['independent data science consultant',\n",
       "  'academic research',\n",
       "  'management/leadership',\n",
       "  'data warehouse/governance'],\n",
       " ['fibreline process engineer',\n",
       "  'management/leadership',\n",
       "  'business solutions',\n",
       "  'software development'],\n",
       " ['data scientist',\n",
       "  'data warehouse/governance',\n",
       "  'client relationship',\n",
       "  'data analytics and reporting'],\n",
       " ['software product engineer',\n",
       "  'marketing',\n",
       "  'client relationship',\n",
       "  'customer support'],\n",
       " ['operation engineer', 'machine learning', 'business solutions', 'marketing'],\n",
       " ['project head',\n",
       "  'management/leadership',\n",
       "  'data analytics and reporting',\n",
       "  'customer support']]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_job_examples(description, vectorizer, model, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

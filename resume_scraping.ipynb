{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from time import sleep\n",
    "from time import time\n",
    "from random import randint\n",
    "from IPython.core.display import clear_output\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy_utils import database_exists, create_database\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls(job_title, start):\n",
    "    \"\"\" get urls from a single page returning results of resume search on indeed.ca\n",
    "    \n",
    "    job_title -- string, eg. 'data+scientist'\n",
    "    start -- index of page to start scraping\n",
    "    \"\"\"\n",
    "    urls = []\n",
    "    \n",
    "    driver = webdriver.Chrome('C:/Users/nalgn/Documents/Insight/project/chromedriver')\n",
    "    driver.get('https://resumes.indeed.com/search?l=canada&q=anytitle%3A%28'+job_title+'%29&start='+str(start))\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    driver.close()                \n",
    "        \n",
    "    # append all urls on a search results page\n",
    "    results = soup.find_all('div', class_='rezemp-ResumeSearchCard')\n",
    "    for result in results:\n",
    "        link = result.find('span').a['href']\n",
    "        urls.append('https://resumes.indeed.com'+str(link))\n",
    "    \n",
    "    return soup, urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_urls(job_title):\n",
    "    \"\"\" navigate through all pages returning search results and get all urls\n",
    "    \n",
    "    job_title -- string, eg. 'data+scientist'\n",
    "    \"\"\"\n",
    "    # Preparing the monitoring of the loop\n",
    "    start_time = time()\n",
    "    requests = 0\n",
    "  \n",
    "    url_list = [] \n",
    "    page_index = 0\n",
    "    while True:\n",
    "        soup, urls = get_urls(job_title, page_index)\n",
    "        url_list.append(urls)\n",
    "        # try again if urls not scraped due to network problem
             if (len(urls)==0):\n",
    "            sleep(4)\n",
    "            continue\n",
    "        \n",
    "        # break if there is no next page\n",
    "        if soup.find('span', class_=\"icl-TextLink icl-TextLink--primary rezemp-pagination-nextbutton\"):\n",
    "            page_index += 50\n",
    "        else:    \n",
    "            break\n",
    "                      \n",
    "        sleep(randint(4, 6))\n",
    "        # monitor requests\n",
    "        requests += 1\n",
    "        elapsed_time = time() - start_time\n",
    "        print('Request:{}; Time: {}; Frequency: {} requests/s'.format(requests, elapsed_time, requests/elapsed_time))\n",
    "        clear_output(wait = True)\n",
    "    \n",
    "    urls = [url for page in url_list for url in page] # url_list is a list of lists, flatten it to a single list\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resume(url):\n",
    "    \"\"\" scrape work experience and education sections from each resume\"\"\"\n",
    "    \n",
    "    driver = webdriver.Chrome('C:/Users/nalgn/Documents/Insight/project/chromedriver')\n",
    "    driver.get(url)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    driver.close()\n",
    "      \n",
    "    resume = soup.find_all('div', class_='rezemp-ResumeDisplaySection') # get all sections of resume\n",
    "                                                                     \n",
    "    # work experience\n",
    "    job_titles = []\n",
    "    job_durations = []\n",
    "    job_descriptions = [] \n",
    "    try:\n",
    "        work = resume[0] # first section of resume\n",
    "        for work_exp in work.find_all('div', class_='rezemp-WorkExperience'):\n",
    "            title = work_exp.span.text\n",
    "            job_titles.append(title)\n",
    "            duration = work_exp.find('div', class_='icl-u-textColor--tertiary').text\n",
    "            job_durations.append(duration)\n",
    "            description = work_exp.find_all('div')[4].text # 5th div under work experience\n",
    "            job_descriptions.append(description)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # education\n",
    "    degrees = []\n",
    "    schools = []\n",
    "    graduation_dates = [] \n",
    "    try:\n",
    "        education = resume[1]\n",
    "        for ed in education.find('div', class_='rezemp-ResumeDisplaySection-content'):\n",
    "            degree = ed.find('span', class_='rezemp-ResumeDisplay-itemTitle').text \n",
    "            degrees.append(degree)\n",
    "            school = ed.find('div', class_='rezemp-ResumeDisplay-university')\n",
    "            school_name = school.find('span', class_='icl-u-textBold').text\n",
    "            schools.append(school_name)\n",
    "            date = ed.find('span', class_='rezemp-ResumeDisplay-date').text\n",
    "            graduation_dates.append(date)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return job_titles, job_durations, job_descriptions, degrees, schools, graduation_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_multiple_resumes(url_list):\n",
    "    \"\"\" get resumes from multiple urls\n",
    "    \n",
    "    url_list = a list of urls\n",
    "    \"\"\"\n",
    "    resumes = []\n",
    "    start_time = time()\n",
    "    requests = 0\n",
    "    for url in url_list:\n",
    "        resume = get_resume(url)\n",
    "        resumes.append(resume)\n",
    "        \n",
    "        # pause the loop             \n",
    "        sleep(randint(4, 6))\n",
    "        # monitor requests\n",
    "        requests += 1\n",
    "        elapsed_time = time() - start_time\n",
    "        print('Request:{}; Time:{}; Frequency: {} requests/s'.format(requests, elapsed_time, requests/elapsed_time))\n",
    "        clear_output(wait = True)\n",
    "    \n",
    "    resumes_df = pd.DataFrame(resumes, columns=['job_titles', 'job_durations', 'job_descriptions',\n",
    "                                               'degrees', 'schools', 'graduation_dates'])\n",
    "    \n",
    "    return resumes_df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting everything together: get all resumes for a job title \n",
    "def mine_resumes(job_title):\n",
    "    \"\"\" get all resumes for a job title\n",
    "    \n",
    "    job_title -- string, eg. 'data+scientist'\n",
    "    \"\"\"\n",
    "    urls = get_all_urls(job_title)\n",
    "    resumes_df = get_multiple_resumes(urls)\n",
    "    resumes_df['job_type'] = job_title\n",
    "    return resumes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request:419; Time:4514.739816665649; Frequency: 0.09280712001460396 requests/s\n"
     ]
    }
   ],
   "source": [
    "data_scientist = mine_resumes('data+scientist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scientist.to_csv('ds_resumes_scraped_1_28.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_sql(df, table_name):\n",
    "    \"\"\"store data in postgresql database\"\"\"\n",
    "    \n",
    "    # set database name, username, and password\n",
    "    dbname = 'data_resumes_db'\n",
    "    username = 'postgres' \n",
    "    password = '1234'\n",
    "    \n",
    "    # engine to connect to database\n",
    "    engine = create_engine('postgres://%s:%s@localhost:5432/%s'%(username, password, dbname))\n",
    "    print(engine.url)\n",
    "    # create database (if it doesn't exist)\n",
    "    if not database_exists(engine.url):\n",
    "        create_database(engine.url)\n",
    "        \n",
    "    df.to_sql(table_name, engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgres://postgres:1234@localhost:5432/data_resumes_db\n"
     ]
    }
   ],
   "source": [
    "save_to_sql(data_scientist, 'resumes_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get n (need to set start and end index) urls from search of a job in canada\n",
    "def get_n_urls(job_title, start, end):\n",
    "    \"\"\" get a custom number of urls for a job by setting start and end page index\n",
    "    \n",
    "    job_title -- string, eg. 'data+scientist'\n",
    "    start -- page index to start scraping, needs to be in multiples of 50\n",
    "    end -- page index to end scraping, needs to be in multiples of 50\n",
    "    \"\"\"\n",
    "    # Preparing the monitoring of the loop\n",
    "    start_time = time()\n",
    "    requests = 0\n",
    "  \n",
    "    url_list = [] \n",
    "    page_index = start\n",
    "    while True:\n",
    "        soup, urls = get_urls(job_title, page_index)\n",
    "        url_list.append(urls)\n",
    "        if (len(urls)==0):\n",
    "            sleep(4)\n",
    "            continue\n",
    "        \n",
    "        page_index += 50\n",
    "        # break if there is no next page\n",
    "        if page_index == end:\n",
    "            break\n",
    "            \n",
    "        # pause the loop             \n",
    "        sleep(randint(4, 6))\n",
    "        # monitor requests\n",
    "        requests += 1\n",
    "        elapsed_time = time() - start_time\n",
    "        print('Request:{}; Time: {}; Frequency: {} requests/s'.format(requests, elapsed_time, requests/elapsed_time))\n",
    "        clear_output(wait = True)\n",
    "    \n",
    "    urls = [url for page in url_list for url in page] # url_list is a list of lists, flatten it to a single list\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request:20; Time: 278.680064201355; Frequency: 0.07176688457179836 requests/s\n"
     ]
    }
   ],
   "source": [
    "urls_data_analyst = get_n_urls('data+analyst', 0, 1050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1050"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(urls_data_analyst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request:1050; Time:11243.377126693726; Frequency: 0.09338831101796968 requests/s\n"
     ]
    }
   ],
   "source": [
    "data_analyst = get_multiple_resumes(urls_data_analyst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_analyst['job_type'] = 'data+analyst'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
